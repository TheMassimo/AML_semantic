{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "OYvFkf3ToiZ1"
      },
      "outputs": [],
      "source": [
        "#@title UTILS\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import numbers\n",
        "import torchvision\n",
        "\n",
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
        "                      max_iter=300, power=0.9):\n",
        "\t\"\"\"Polynomial decay of learning rate\n",
        "\t\t:param init_lr is base learning rate\n",
        "\t\t:param iter is a current iteration\n",
        "\t\t:param lr_decay_iter how frequently decay occurs, default is 1\n",
        "\t\t:param max_iter is number of maximum iterations\n",
        "\t\t:param power is a polymomial power\n",
        "\n",
        "\t\"\"\"\n",
        "\t# if iter % lr_decay_iter or iter > max_iter:\n",
        "\t# \treturn optimizer\n",
        "\n",
        "\tlr = init_lr*(1 - iter/max_iter)**power\n",
        "\toptimizer.param_groups[0]['lr'] = lr\n",
        "\treturn lr\n",
        "\t# return lr\n",
        "\n",
        "def get_label_info(csv_path):\n",
        "\t# return label -> {label_name: [r_value, g_value, b_value, ...}\n",
        "\tann = pd.read_csv(csv_path)\n",
        "\tlabel = {}\n",
        "\tfor iter, row in ann.iterrows():\n",
        "\t\tlabel_name = row['name']\n",
        "\t\tr = row['r']\n",
        "\t\tg = row['g']\n",
        "\t\tb = row['b']\n",
        "\t\tclass_11 = row['class_11']\n",
        "\t\tlabel[label_name] = [int(r), int(g), int(b), class_11]\n",
        "\treturn label\n",
        "\n",
        "def one_hot_it(label, label_info):\n",
        "\t# return semantic_map -> [H, W]\n",
        "\tsemantic_map = np.zeros(label.shape[:-1])\n",
        "\tfor index, info in enumerate(label_info):\n",
        "\t\tcolor = label_info[info]\n",
        "\t\t# colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
        "\t\tequality = np.equal(label, color)\n",
        "\t\tclass_map = np.all(equality, axis=-1)\n",
        "\t\tsemantic_map[class_map] = index\n",
        "\t\t# semantic_map.append(class_map)\n",
        "\t# semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\treturn semantic_map\n",
        "\n",
        "\n",
        "def one_hot_it_v11(label, label_info):\n",
        "\t# return semantic_map -> [H, W, class_num]\n",
        "\tsemantic_map = np.zeros(label.shape[:-1])\n",
        "\t# from 0 to 11, and 11 means void\n",
        "\tclass_index = 0\n",
        "\tfor index, info in enumerate(label_info):\n",
        "\t\tcolor = label_info[info][:3]\n",
        "\t\tclass_11 = label_info[info][3]\n",
        "\t\tif class_11 == 1:\n",
        "\t\t\t# colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
        "\t\t\tequality = np.equal(label, color)\n",
        "\t\t\tclass_map = np.all(equality, axis=-1)\n",
        "\t\t\t# semantic_map[class_map] = index\n",
        "\t\t\tsemantic_map[class_map] = class_index\n",
        "\t\t\tclass_index += 1\n",
        "\t\telse:\n",
        "\t\t\tequality = np.equal(label, color)\n",
        "\t\t\tclass_map = np.all(equality, axis=-1)\n",
        "\t\t\tsemantic_map[class_map] = 11\n",
        "\treturn semantic_map\n",
        "\n",
        "def one_hot_it_v11_dice(label, label_info):\n",
        "\t# return semantic_map -> [H, W, class_num]\n",
        "\tsemantic_map = []\n",
        "\tvoid = np.zeros(label.shape[:2])\n",
        "\tfor index, info in enumerate(label_info):\n",
        "\t\tcolor = label_info[info][:3]\n",
        "\t\tclass_11 = label_info[info][3]\n",
        "\t\tif class_11 == 1:\n",
        "\t\t\t# colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
        "\t\t\tequality = np.equal(label, color)\n",
        "\t\t\tclass_map = np.all(equality, axis=-1)\n",
        "\t\t\t# semantic_map[class_map] = index\n",
        "\t\t\tsemantic_map.append(class_map)\n",
        "\t\telse:\n",
        "\t\t\tequality = np.equal(label, color)\n",
        "\t\t\tclass_map = np.all(equality, axis=-1)\n",
        "\t\t\tvoid[class_map] = 1\n",
        "\tsemantic_map.append(void)\n",
        "\tsemantic_map = np.stack(semantic_map, axis=-1).astype(np.float)\n",
        "\treturn semantic_map\n",
        "\n",
        "def reverse_one_hot(image):\n",
        "\t\"\"\"\n",
        "\tTransform a 2D array in one-hot format (depth is num_classes),\n",
        "\tto a 2D array with only 1 channel, where each pixel value is\n",
        "\tthe classified class key.\n",
        "\n",
        "\t# Arguments\n",
        "\t\timage: The one-hot format image\n",
        "\n",
        "\t# Returns\n",
        "\t\tA 2D array with the same width and height as the input, but\n",
        "\t\twith a depth size of 1, where each pixel value is the classified\n",
        "\t\tclass key.\n",
        "\t\"\"\"\n",
        "\t# w = image.shape[0]\n",
        "\t# h = image.shape[1]\n",
        "\t# x = np.zeros([w,h,1])\n",
        "\n",
        "\t# for i in range(0, w):\n",
        "\t#     for j in range(0, h):\n",
        "\t#         index, value = max(enumerate(image[i, j, :]), key=operator.itemgetter(1))\n",
        "\t#         x[i, j] = index\n",
        "\timage = image.permute(1, 2, 0)\n",
        "\tx = torch.argmax(image, dim=-1)\n",
        "\treturn x\n",
        "\n",
        "\n",
        "def colour_code_segmentation(image, label_values):\n",
        "\t\"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "\n",
        "\t# w = image.shape[0]\n",
        "\t# h = image.shape[1]\n",
        "\t# x = np.zeros([w,h,3])\n",
        "\t# colour_codes = label_values\n",
        "\t# for i in range(0, w):\n",
        "\t#     for j in range(0, h):\n",
        "\t#         x[i, j, :] = colour_codes[int(image[i, j])]\n",
        "\tlabel_values = [label_values[key][:3] for key in label_values if label_values[key][3] == 1]\n",
        "\tlabel_values.append([0, 0, 0])\n",
        "\tcolour_codes = np.array(label_values)\n",
        "\tx = colour_codes[image.astype(int)]\n",
        "\n",
        "\treturn x\n",
        "\n",
        "def compute_global_accuracy(pred, label):\n",
        "\tpred = pred.flatten()\n",
        "\tlabel = label.flatten()\n",
        "\ttotal = len(label)\n",
        "\tcount = 0.0\n",
        "\tfor i in range(total):\n",
        "\t\tif pred[i] == label[i]:\n",
        "\t\t\tcount = count + 1.0\n",
        "\treturn float(count) / float(total)\n",
        "\n",
        "def fast_hist(a, b, n):\n",
        "\t'''\n",
        "\ta and b are predict and mask respectively\n",
        "\tn is the number of classes\n",
        "\t'''\n",
        "\tk = (a >= 0) & (a < n)\n",
        "\treturn np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "\n",
        "def per_class_iu(hist):\n",
        "\tepsilon = 1e-5\n",
        "\treturn (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
        "\n",
        "class RandomCrop(object):\n",
        "\t\"\"\"Crop the given PIL Image at a random location.\n",
        "\n",
        "\tArgs:\n",
        "\t\tsize (sequence or int): Desired output size of the crop. If size is an\n",
        "\t\t\tint instead of sequence like (h, w), a square crop (size, size) is\n",
        "\t\t\tmade.\n",
        "\t\tpadding (int or sequence, optional): Optional padding on each border\n",
        "\t\t\tof the image. Default is 0, i.e no padding. If a sequence of length\n",
        "\t\t\t4 is provided, it is used to pad left, top, right, bottom borders\n",
        "\t\t\trespectively.\n",
        "\t\tpad_if_needed (boolean): It will pad the image if smaller than the\n",
        "\t\t\tdesired size to avoid raising an exception.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, size, seed, padding=0, pad_if_needed=False):\n",
        "\t\tif isinstance(size, numbers.Number):\n",
        "\t\t\tself.size = (int(size), int(size))\n",
        "\t\telse:\n",
        "\t\t\tself.size = size\n",
        "\t\tself.padding = padding\n",
        "\t\tself.pad_if_needed = pad_if_needed\n",
        "\t\tself.seed = seed\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_params(img, output_size, seed):\n",
        "\t\t\"\"\"Get parameters for ``crop`` for a random crop.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\timg (PIL Image): Image to be cropped.\n",
        "\t\t\toutput_size (tuple): Expected output size of the crop.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
        "\t\t\"\"\"\n",
        "\t\trandom.seed(seed)\n",
        "\t\tw, h = img.size\n",
        "\t\tth, tw = output_size\n",
        "\t\tif w == tw and h == th:\n",
        "\t\t\treturn 0, 0, h, w\n",
        "\t\ti = random.randint(0, h - th)\n",
        "\t\tj = random.randint(0, w - tw)\n",
        "\t\treturn i, j, th, tw\n",
        "\n",
        "\tdef __call__(self, img):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\timg (PIL Image): Image to be cropped.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tPIL Image: Cropped image.\n",
        "\t\t\"\"\"\n",
        "\t\tif self.padding > 0:\n",
        "\t\t\timg = torchvision.transforms.functional.pad(img, self.padding)\n",
        "\n",
        "\t\t# pad the width if needed\n",
        "\t\tif self.pad_if_needed and img.size[0] < self.size[1]:\n",
        "\t\t\timg = torchvision.transforms.functional.pad(img, (int((1 + self.size[1] - img.size[0]) / 2), 0))\n",
        "\t\t# pad the height if needed\n",
        "\t\tif self.pad_if_needed and img.size[1] < self.size[0]:\n",
        "\t\t\timg = torchvision.transforms.functional.pad(img, (0, int((1 + self.size[0] - img.size[1]) / 2)))\n",
        "\n",
        "\t\ti, j, h, w = self.get_params(img, self.size, self.seed)\n",
        "\n",
        "\t\treturn torchvision.transforms.functional.crop(img, i, j, h, w)\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n",
        "\n",
        "def cal_miou(miou_list, csv_path):\n",
        "\t# return label -> {label_name: [r_value, g_value, b_value, ...}\n",
        "\tann = pd.read_csv(csv_path)\n",
        "\tmiou_dict = {}\n",
        "\tcnt = 0\n",
        "\tfor iter, row in ann.iterrows():\n",
        "\t\tlabel_name = row['name']\n",
        "\t\tclass_11 = int(row['class_11'])\n",
        "\t\tif class_11 == 1:\n",
        "\t\t\tmiou_dict[label_name] = miou_list[cnt]\n",
        "\t\t\tcnt += 1\n",
        "\treturn miou_dict, np.mean(miou_list)\n",
        "\n",
        "class OHEM_CrossEntroy_Loss(nn.Module):\n",
        "\tdef __init__(self, threshold, keep_num):\n",
        "\t\tsuper(OHEM_CrossEntroy_Loss, self).__init__()\n",
        "\t\tself.threshold = threshold\n",
        "\t\tself.keep_num = keep_num\n",
        "\t\tself.loss_function = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "\tdef forward(self, output, target):\n",
        "\t\tloss = self.loss_function(output, target).view(-1)\n",
        "\t\tloss, loss_index = torch.sort(loss, descending=True)\n",
        "\t\tthreshold_in_keep_num = loss[self.keep_num]\n",
        "\t\tif threshold_in_keep_num > self.threshold:\n",
        "\t\t\tloss = loss[loss>self.threshold]\n",
        "\t\telse:\n",
        "\t\t\tloss = loss[:self.keep_num]\n",
        "\t\treturn torch.mean(loss)\n",
        "\n",
        "def group_weight(weight_group, module, norm_layer, lr):\n",
        "\tgroup_decay = []\n",
        "\tgroup_no_decay = []\n",
        "\tfor m in module.modules():\n",
        "\t\tif isinstance(m, nn.Linear):\n",
        "\t\t\tgroup_decay.append(m.weight)\n",
        "\t\t\tif m.bias is not None:\n",
        "\t\t\t\tgroup_no_decay.append(m.bias)\n",
        "\t\telif isinstance(m, (nn.Conv2d, nn.Conv3d)):\n",
        "\t\t\tgroup_decay.append(m.weight)\n",
        "\t\t\tif m.bias is not None:\n",
        "\t\t\t\tgroup_no_decay.append(m.bias)\n",
        "\t\telif isinstance(m, norm_layer) or isinstance(m, nn.GroupNorm):\n",
        "\t\t\tif m.weight is not None:\n",
        "\t\t\t\tgroup_no_decay.append(m.weight)\n",
        "\t\t\tif m.bias is not None:\n",
        "\t\t\t\tgroup_no_decay.append(m.bias)\n",
        "\n",
        "\tassert len(list(module.parameters())) == len(group_decay) + len(\n",
        "\t\tgroup_no_decay)\n",
        "\tweight_group.append(dict(params=group_decay, lr=lr))\n",
        "\tweight_group.append(dict(params=group_no_decay, weight_decay=.0, lr=lr))\n",
        "\treturn weight_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auN8gCpR6p9k"
      },
      "source": [
        "TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "iImgIkUhN4qi"
      },
      "outputs": [],
      "source": [
        "#@title Train\n",
        "\n",
        "\n",
        "#!/usr/bin/python\n",
        "# -*- encoding: utf-8 -*-\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "#from stdcnet import STDCNet\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "'''\n",
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            label = label.type(torch.LongTensor)\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "'''\n",
        "\n",
        "\n",
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    model.eval()  # Modalità di validazione\n",
        "    model.to(device)  # Sposta il modello sulla GPU\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            data = data.to(device)  # Sposta i dati sulla GPU\n",
        "            label = label.to(device)  # Sposta le etichette sulla GPU\n",
        "\n",
        "\n",
        "            # get RGB predict image\n",
        "            predict, _, _ = model(data)\n",
        "            predict = predict.squeeze(0)\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            # there is no need to transform the one-hot array to visual RGB array\n",
        "            # predict = colour_code_segmentation(np.array(predict), label_info)\n",
        "            # label = colour_code_segmentation(np.array(label), label_info)\n",
        "            precision_record.append(precision)\n",
        "\n",
        "        precision = np.mean(precision_record)\n",
        "        miou_list = per_class_iu(hist)\n",
        "        miou = np.mean(miou_list)\n",
        "        print('precision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        print(f'mIoU per class: {miou_list}')\n",
        "\n",
        "        return precision, miou\n",
        "\n",
        "\n",
        "def train(args, model, optimizer, dataloader_train, dataloader_val):\n",
        "    writer = SummaryWriter(comment=''.format(args.optimizer))\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "    model.to(device)  # Sposta il modello sulla GPU\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch, max_iter=args.num_epochs)\n",
        "        model.train()\n",
        "        tq = tqdm(total=len(dataloader_train) * args.batch_size)\n",
        "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
        "        loss_record = []\n",
        "        for i, (data, label) in enumerate(dataloader_train):\n",
        "            data = data.to(device)  # Sposta i dati sulla GPU\n",
        "            label = label.long().to(device)  # Sposta le etichette sulla GPU\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with amp.autocast():\n",
        "                output, out16, out32 = model(data)\n",
        "                loss1 = loss_func(output, label.squeeze(1))\n",
        "                loss2 = loss_func(out16, label.squeeze(1))\n",
        "                loss3 = loss_func(out32, label.squeeze(1))\n",
        "                loss = loss1 + loss2 + loss3\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            tq.set_postfix(loss='%.6f' % loss)\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_step', loss, step)\n",
        "            loss_record.append(loss.item())\n",
        "        tq.close()\n",
        "        loss_train_mean = np.mean(loss_record)\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
        "        print('loss for train : %f' % (loss_train_mean))\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            # Controllo se il modello è avvolto in DataParallel o DistributedDataParallel\n",
        "            if isinstance(model, torch.nn.DataParallel) or isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "                torch.save(model.module.state_dict(), os.path.join(args.save_model_path, 'latest.pth'))\n",
        "            else: \n",
        "                torch.save(model.state_dict(), os.path.join(args.save_model_path, 'latest.pth'))\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != 0:\n",
        "            precision, miou = val(args, model, dataloader_val)\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                if isinstance(model, torch.nn.DataParallel) or isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "                    torch.save(model.module.state_dict(), os.path.join(args.save_model_path, 'best.pth'))\n",
        "                else:\n",
        "                    torch.save(model.state_dict(), os.path.join(args.save_model_path, 'best.pth'))\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)\n",
        "\n",
        "\n",
        "        '''\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), os.path.join(args.save_model_path, 'latest.pth'))\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != 0:\n",
        "            precision, miou = val(args, model, dataloader_val)\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), os.path.join(args.save_model_path, 'best.pth'))\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)\n",
        "            '''\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parse = argparse.ArgumentParser()\n",
        "\n",
        "    parse.add_argument('--root',\n",
        "                       dest='root',\n",
        "                       type=str,\n",
        "                       default='../Datasets/Cityscapes',\n",
        "                       help='Percorso radice per i dati di Cityscapes')\n",
        "    parse.add_argument('--root_source',\n",
        "                       dest='root_source',\n",
        "                       type=str,\n",
        "                       default='../Datasets/GTA5',\n",
        "                       help='Percorso radice per i dati di GTA5 (se utilizzato come sorgente)')\n",
        "    parse.add_argument('--root_target',\n",
        "                       dest='root_target',\n",
        "                       type=str,\n",
        "                       default='../Datasets/Cityscapes',\n",
        "                       help='Percorso radice per i dati di Cityscapes (se utilizzato come target)')\n",
        "    parse.add_argument('--dataset',\n",
        "                       dest='dataset',\n",
        "                       type=str,\n",
        "                       default='Cityspaces',\n",
        "                       help='Seleziona il dataset tra GTAV e Cityspaces')\n",
        "\n",
        "    parse.add_argument('--backbone',\n",
        "                        dest='backbone',\n",
        "                        type=str,\n",
        "                        default='STDCNet813M',\n",
        "                        help='Tipo di backbone del modello')\n",
        "    parse.add_argument('--pretrain_path',\n",
        "                       dest='pretrain_path',\n",
        "                       type=str,\n",
        "                       default='pretrained/STDCNet813M_73.91',\n",
        "                       help='Percorso del modello pre-addestrato')\n",
        "    parse.add_argument('--use_conv_last',\n",
        "                       dest='use_conv_last',\n",
        "                       type=str2bool,\n",
        "                       default=False,\n",
        "                       help='Se utilizzare un\\'ulteriore convoluzione alla fine del modello')\n",
        "\n",
        "    parse.add_argument('--num_epochs',\n",
        "                       type=int,\n",
        "                       default=50,\n",
        "                       help='Numero di epoche per l\\'addestramento')\n",
        "    parse.add_argument('--epoch_start_i',\n",
        "                       type=int,\n",
        "                       default=0,\n",
        "                       help='Inizia il conteggio delle epoche da questo numero')\n",
        "    parse.add_argument('--checkpoint_step',\n",
        "                       type=int,\n",
        "                       default=10,\n",
        "                       help='Frequenza per il salvataggio dei checkpoint (in epoche)')\n",
        "    parse.add_argument('--validation_step',\n",
        "                       type=int,\n",
        "                       default=5,\n",
        "                       help='Frequenza per l\\'esecuzione della validazione (in epoche)')\n",
        "    parse.add_argument('--crop_height',\n",
        "                       type=int,\n",
        "                       default=512,\n",
        "                       help='Altezza delle immagini dopo il ritaglio/ridimensionamento')\n",
        "    parse.add_argument('--crop_width',\n",
        "                       type=int,\n",
        "                       default=1024,\n",
        "                       help='Larghezza delle immagini dopo il ritaglio/ridimensionamento')\n",
        "    parse.add_argument('--batch_size',\n",
        "                       type=int,\n",
        "                       default=8,\n",
        "                       help='Numero di immagini in ogni batch')\n",
        "    parse.add_argument('--learning_rate',\n",
        "                       type=float,\n",
        "                       default=0.01,\n",
        "                       help='Tasso di apprendimento usato per l\\'addestramento')\n",
        "    parse.add_argument('--num_workers',\n",
        "                       type=int,\n",
        "                       default=0,\n",
        "                       help='Numero di worker per il caricamento dei dati')\n",
        "    parse.add_argument('--num_classes',\n",
        "                       type=int,\n",
        "                       default=19,\n",
        "                       help='Numero di classi nel dataset (inclusa la classe di sfondo)')\n",
        "    parse.add_argument('--cuda',\n",
        "                       type=str,\n",
        "                       default='0',\n",
        "                       help='ID della GPU utilizzata per l\\'addestramento')\n",
        "    parse.add_argument('--use_gpu',\n",
        "                       type=bool,\n",
        "                       default=True,\n",
        "                       help='Se utilizzare la GPU per l\\'addestramento')\n",
        "    parse.add_argument('--save_model_path',\n",
        "                   type=str,\n",
        "                   default='./model_checkpoints',\n",
        "                   help='Percorso per salvare il modello')\n",
        "    parse.add_argument('--optimizer',\n",
        "                       type=str,\n",
        "                       default='sgd',\n",
        "                       help='Tipo di ottimizzatore (supporta rmsprop, sgd, adam)')\n",
        "    parse.add_argument('--loss',\n",
        "                       type=str,\n",
        "                       default='crossentropy',\n",
        "                       help='Funzione di perdita')\n",
        "    parse.add_argument('--iter_size',\n",
        "                       type=int,\n",
        "                       default=1,\n",
        "                       help='Accumulare i gradienti per ITER_SIZE iterazioni')\n",
        "    parse.add_argument('--domain_shift',\n",
        "                       type=bool,\n",
        "                       default=False,\n",
        "                       help='Testare lo shift di dominio da GTAV a Cityscapes')\n",
        "    parse.add_argument('--domain_adaptation',\n",
        "                       type=bool,\n",
        "                       default=False,\n",
        "                       help='Addestrare l\\'adattamento del dominio da GTAV a Cityscapes')\n",
        "    parse.add_argument('--momentum',\n",
        "                       type=float,\n",
        "                       default=0.9,\n",
        "                       help='Componente di momento dell\\'ottimizzatore')\n",
        "    parse.add_argument('--weight_decay',\n",
        "                       type=float,\n",
        "                       default=5e-4,\n",
        "                       help='Peso del termine di decadimento nell\\'ottimizzatore')\n",
        "    parse.add_argument('--aug_type',\n",
        "                       type=str,\n",
        "                       default=None,\n",
        "                       help='Tipo di Data Augmentation da applicare')\n",
        "    parse.add_argument('--mode',\n",
        "                       dest='mode',\n",
        "                       type=str,\n",
        "                       default='train',\n",
        "                       help='Modalità di esecuzione (train/val/test)')\n",
        "\n",
        "    # Ignora gli argomenti sconosciuti\n",
        "    args, unknown = parse.parse_known_args()\n",
        "    return args\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "aDLewjjEo125"
      },
      "outputs": [],
      "source": [
        "#@title stdcnet\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "\n",
        "class ConvX(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel=3, stride=1):\n",
        "        super(ConvX, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel // 2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn(self.conv(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class AddBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(AddBottleneck, self).__init__()\n",
        "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(out_planes // 2, out_planes // 2, kernel_size=3, stride=2, padding=1, groups=out_planes // 2,\n",
        "                          bias=False),\n",
        "                nn.BatchNorm2d(out_planes // 2),\n",
        "            )\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=2, padding=1, groups=in_planes, bias=False),\n",
        "                nn.BatchNorm2d(in_planes),\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(ConvX(in_planes, out_planes // 2, kernel=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(ConvX(out_planes // 2, out_planes // 2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(ConvX(out_planes // 2, out_planes // 4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(\n",
        "                    ConvX(out_planes // int(math.pow(2, idx)), out_planes // int(math.pow(2, idx + 1))))\n",
        "            else:\n",
        "                self.conv_list.append(ConvX(out_planes // int(math.pow(2, idx)), out_planes // int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out = x\n",
        "\n",
        "        for idx, conv in enumerate(self.conv_list):\n",
        "            if idx == 0 and self.stride == 2:\n",
        "                out = self.avd_layer(conv(out))\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "\n",
        "        if self.stride == 2:\n",
        "            x = self.skip(x)\n",
        "\n",
        "        return torch.cat(out_list, dim=1) + x\n",
        "\n",
        "\n",
        "class CatBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(CatBottleneck, self).__init__()\n",
        "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(out_planes // 2, out_planes // 2, kernel_size=3, stride=2, padding=1, groups=out_planes // 2,\n",
        "                          bias=False),\n",
        "                nn.BatchNorm2d(out_planes // 2),\n",
        "            )\n",
        "            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(ConvX(in_planes, out_planes // 2, kernel=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(ConvX(out_planes // 2, out_planes // 2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(ConvX(out_planes // 2, out_planes // 4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(\n",
        "                    ConvX(out_planes // int(math.pow(2, idx)), out_planes // int(math.pow(2, idx + 1))))\n",
        "            else:\n",
        "                self.conv_list.append(ConvX(out_planes // int(math.pow(2, idx)), out_planes // int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out1 = self.conv_list[0](x)\n",
        "\n",
        "        for idx, conv in enumerate(self.conv_list[1:]):\n",
        "            if idx == 0:\n",
        "                if self.stride == 2:\n",
        "                    out = conv(self.avd_layer(out1))\n",
        "                else:\n",
        "                    out = conv(out1)\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "\n",
        "        if self.stride == 2:\n",
        "            out1 = self.skip(out1)\n",
        "        out_list.insert(0, out1)\n",
        "\n",
        "        out = torch.cat(out_list, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# STDC1Net\n",
        "class STDCNet813(nn.Module):\n",
        "    def __init__(self, base=64, layers=[2, 2, 2], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20,\n",
        "                 pretrain_model='', use_conv_last=False):\n",
        "        super(STDCNet813, self).__init__()\n",
        "        if type == \"cat\":\n",
        "            block = CatBottleneck\n",
        "        elif type == \"add\":\n",
        "            block = AddBottleneck\n",
        "        self.use_conv_last = use_conv_last\n",
        "        self.features = self._make_layers(base, layers, block_num, block)\n",
        "        self.conv_last = ConvX(base * 16, max(1024, base * 16), 1, 1)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(max(1024, base * 16), max(1024, base * 16), bias=False)\n",
        "        self.bn = nn.BatchNorm1d(max(1024, base * 16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(max(1024, base * 16), num_classes, bias=False)\n",
        "\n",
        "        self.x2 = nn.Sequential(self.features[:1])\n",
        "        self.x4 = nn.Sequential(self.features[1:2])\n",
        "        self.x8 = nn.Sequential(self.features[2:4])\n",
        "        self.x16 = nn.Sequential(self.features[4:6])\n",
        "        self.x32 = nn.Sequential(self.features[6:])\n",
        "\n",
        "        if pretrain_model:\n",
        "            print('use pretrain model {}'.format(pretrain_model))\n",
        "            self.init_weight(pretrain_model)\n",
        "        else:\n",
        "            self.init_params()\n",
        "\n",
        "    def init_weight(self, pretrain_model):\n",
        "\n",
        "        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            self_state_dict.update({k: v})\n",
        "        self.load_state_dict(self_state_dict)\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layers(self, base, layers, block_num, block):\n",
        "        features = []\n",
        "        features += [ConvX(3, base // 2, 3, 2)]\n",
        "        features += [ConvX(base // 2, base, 3, 2)]\n",
        "\n",
        "        for i, layer in enumerate(layers):\n",
        "            for j in range(layer):\n",
        "                if i == 0 and j == 0:\n",
        "                    features.append(block(base, base * 4, block_num, 2))\n",
        "                elif j == 0:\n",
        "                    features.append(block(base * int(math.pow(2, i + 1)), base * int(math.pow(2, i + 2)), block_num, 2))\n",
        "                else:\n",
        "                    features.append(block(base * int(math.pow(2, i + 2)), base * int(math.pow(2, i + 2)), block_num, 1))\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat2 = self.x2(x)\n",
        "        feat4 = self.x4(feat2)\n",
        "        feat8 = self.x8(feat4)\n",
        "        feat16 = self.x16(feat8)\n",
        "        feat32 = self.x32(feat16)\n",
        "        if self.use_conv_last:\n",
        "            feat32 = self.conv_last(feat32)\n",
        "\n",
        "        return feat2, feat4, feat8, feat16, feat32\n",
        "\n",
        "    def forward_impl(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.conv_last(out).pow(2)\n",
        "        out = self.gap(out).flatten(1)\n",
        "        out = self.fc(out)\n",
        "        # out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        # out = self.relu(self.bn(self.fc(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "jG44twYnor8w"
      },
      "outputs": [],
      "source": [
        "#@title model stages\n",
        "\n",
        "\n",
        "#!/usr/bin/python\n",
        "# -*- encoding: utf-8 -*-\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_chan,\n",
        "                              out_chan,\n",
        "                              kernel_size=ks,\n",
        "                              stride=stride,\n",
        "                              padding=padding,\n",
        "                              bias=False)\n",
        "        # self.bn = BatchNorm2d(out_chan)\n",
        "        self.bn = BatchNorm2d(out_chan)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "\n",
        "class BiSeNetOutput(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n",
        "        super(BiSeNetOutput, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(AttentionRefinementModule, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size=1, bias=False)\n",
        "        # self.bn_atten = BatchNorm2d(out_chan)\n",
        "        self.bn_atten = BatchNorm2d(out_chan)\n",
        "\n",
        "        self.sigmoid_atten = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv_atten(atten)\n",
        "        atten = self.bn_atten(atten)\n",
        "        atten = self.sigmoid_atten(atten)\n",
        "        out = torch.mul(feat, atten)\n",
        "        return out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "\n",
        "class ContextPath(nn.Module):\n",
        "    def __init__(self, backbone='CatNetSmall', pretrain_model='', use_conv_last=False, *args, **kwargs):\n",
        "        super(ContextPath, self).__init__()\n",
        "\n",
        "        self.backbone = STDCNet813(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n",
        "        self.arm16 = AttentionRefinementModule(512, 128)\n",
        "        inplanes = 1024\n",
        "        if use_conv_last:\n",
        "            inplanes = 1024\n",
        "        self.arm32 = AttentionRefinementModule(inplanes, 128)\n",
        "        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "        self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H0, W0 = x.size()[2:]\n",
        "\n",
        "        feat2, feat4, feat8, feat16, feat32 = self.backbone(x)\n",
        "        H8, W8 = feat8.size()[2:]\n",
        "        H16, W16 = feat16.size()[2:]\n",
        "        H32, W32 = feat32.size()[2:]\n",
        "\n",
        "        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n",
        "\n",
        "        avg = self.conv_avg(avg)\n",
        "        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n",
        "\n",
        "        feat32_arm = self.arm32(feat32)\n",
        "        feat32_sum = feat32_arm + avg_up\n",
        "        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n",
        "        feat32_up = self.conv_head32(feat32_up)\n",
        "\n",
        "        feat16_arm = self.arm16(feat16)\n",
        "        feat16_sum = feat16_arm + feat32_up\n",
        "        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n",
        "        feat16_up = self.conv_head16(feat16_up)\n",
        "\n",
        "        return feat2, feat4, feat8, feat16, feat16_up, feat32_up  # x8, x16\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class FeatureFusionModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(FeatureFusionModule, self).__init__()\n",
        "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
        "        self.conv1 = nn.Conv2d(out_chan,\n",
        "                               out_chan // 4,\n",
        "                               kernel_size=1,\n",
        "                               stride=1,\n",
        "                               padding=0,\n",
        "                               bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_chan // 4,\n",
        "                               out_chan,\n",
        "                               kernel_size=1,\n",
        "                               stride=1,\n",
        "                               padding=0,\n",
        "                               bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, fsp, fcp):\n",
        "        fcat = torch.cat([fsp, fcp], dim=1)\n",
        "        feat = self.convblk(fcat)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv1(atten)\n",
        "        atten = self.relu(atten)\n",
        "        atten = self.conv2(atten)\n",
        "        atten = self.sigmoid(atten)\n",
        "        feat_atten = torch.mul(feat, atten)\n",
        "        feat_out = feat_atten + feat\n",
        "        return feat_out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class BiSeNet(nn.Module):\n",
        "    def __init__(self, backbone, n_classes, pretrain_model='', use_boundary_2=False, use_boundary_4=False,\n",
        "                 use_boundary_8=False, use_boundary_16=False, use_conv_last=False, heat_map=False, *args, **kwargs):\n",
        "        super(BiSeNet, self).__init__()\n",
        "\n",
        "        # self.heat_map = heat_map\n",
        "        self.cp = ContextPath(backbone, pretrain_model, use_conv_last=use_conv_last)\n",
        "\n",
        "        conv_out_inplanes = 128\n",
        "        sp2_inplanes = 32\n",
        "        sp4_inplanes = 64\n",
        "        sp8_inplanes = 256\n",
        "        sp16_inplanes = 512\n",
        "        inplane = sp8_inplanes + conv_out_inplanes\n",
        "\n",
        "\n",
        "        self.ffm = FeatureFusionModule(inplane, 256)\n",
        "        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n",
        "        self.conv_out16 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
        "        self.conv_out32 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.size()[2:]\n",
        "\n",
        "        feat_res2, feat_res4, feat_res8, feat_res16, feat_cp8, feat_cp16 = self.cp(x)\n",
        "\n",
        "        feat_fuse = self.ffm(feat_res8, feat_cp8)\n",
        "\n",
        "        feat_out = self.conv_out(feat_fuse)\n",
        "        feat_out16 = self.conv_out16(feat_cp8)\n",
        "        feat_out32 = self.conv_out32(feat_cp16)\n",
        "\n",
        "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
        "\n",
        "        return feat_out, feat_out16, feat_out32\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
        "        for name, child in self.named_children():\n",
        "            child_wd_params, child_nowd_params = child.get_params()\n",
        "            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n",
        "                lr_mul_wd_params += child_wd_params\n",
        "                lr_mul_nowd_params += child_nowd_params\n",
        "            else:\n",
        "                wd_params += child_wd_params\n",
        "                nowd_params += child_nowd_params\n",
        "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_YWjHiWi8_gn"
      },
      "outputs": [],
      "source": [
        "#cityscape\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\t\n",
        "\n",
        "class CityScapesDataset(Dataset):\n",
        "    def __init__(self, root_dir=\"\", mode='train', dimension= (2048, 1024), transform=None):\n",
        "        \"\"\"\n",
        "        Inizializza il dataset CityScapes.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Directory principale del dataset.\n",
        "            mode (str): Modalità del dataset ('train', 'val', 'test').\n",
        "            dimension (int, int): Altezza e Larghezza a cui ridimensionare le immagini.\n",
        "        \"\"\"\n",
        "        super(CityScapesDataset, self).__init__()\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.mode = mode  # mode can be 'train', 'val', or 'test'\n",
        "        self.transform = transform\n",
        "        self.resize = dimension\n",
        "\n",
        "        # Cityscapes directory structure\n",
        "        self.images_dir = os.path.join(self.root_dir, 'images', self.mode)\n",
        "        self.labels_dir = os.path.join(self.root_dir, 'gtFine', self.mode)\n",
        "\n",
        "        # Get list of all image and label files\n",
        "        self.images_paths = []\n",
        "        self.labels_paths = []\n",
        "\n",
        "        # Trasformazioni da applicare alle immagini e alle etichette\n",
        "        normalizer = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        self.to_tensor = transforms.Compose([\n",
        "            transforms.Resize(self.resize),\n",
        "            transforms.ToTensor(),\n",
        "            normalizer  # Aggiungi la normalizzazione qui\n",
        "        ])\n",
        "        self.to_tensor_label = transforms.Compose([\n",
        "            transforms.Resize(self.resize, interpolation=Image.NEAREST),\n",
        "            transforms.PILToTensor()\n",
        "        ])\n",
        "\n",
        "        #caricamento percorsi di immagini ed etichette\n",
        "        self.images_paths = self.__get_file_paths__(self.images_dir, ['.png', '.jpg', '.jpeg'])\n",
        "        #escludiamo color per prendere solo le maschere\n",
        "        self.labels_paths = self.__get_file_paths__(self.labels_dir, ['.png'], include_keywords=['train'])\n",
        "\n",
        "        # Creazione del DataFrame con percorsi di immagini e etichette\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image_path\": sorted(self.images_paths),\n",
        "            \"label_path\": sorted(self.labels_paths)\n",
        "        })\n",
        "\n",
        "    def __get_file_paths__(self, dir_path, valid_extensions, include_keywords=None):\n",
        "        \"\"\"\n",
        "        Restituisce una lista di percorsi di file validi in una directory.\n",
        "\n",
        "        Args:\n",
        "            dir_path (str): Percorso della directory da esaminare.\n",
        "            valid_extensions (list): Estensioni valide per i file.\n",
        "            include_keywords (list): Parole chiave da includere nei nomi dei file (opzionale).\n",
        "\n",
        "        Returns:\n",
        "            list: Lista di percorsi dei file validi.\n",
        "        \"\"\"\n",
        "        file_paths = []\n",
        "        for root, _, files in os.walk(dir_path):\n",
        "            for file in files:\n",
        "                if any(file.lower().endswith(ext) for ext in valid_extensions):\n",
        "                    # Se include_keywords è None, includi tutte le immagini\n",
        "                    if include_keywords is None or any(keyword in file.lower() for keyword in include_keywords):\n",
        "                        file_paths.append(os.path.join(root, file))\n",
        "        return file_paths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Restituisce una coppia immagine-etichetta trasformata.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Indice della coppia immagine-etichetta.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Coppia (immagine, etichetta) entrambe come tensori.\n",
        "        \"\"\"\n",
        "        image_path = self.data.iloc[idx][\"image_path\"]\n",
        "        label_path = self.data.iloc[idx][\"label_path\"]\n",
        "\n",
        "        image = pil_loader(image_path)\n",
        "        label = Image.open(label_path)\n",
        "\n",
        "        image = self.to_tensor(image)\n",
        "        label = self.to_tensor_label(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        #restituisce il numero di coppie immagine-etichetta\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Gta5Dataset(Dataset):\n",
        "    def __init__(self, root, dimension=(1024, 512)):\n",
        "        super(Gta5Dataset, self).__init__()\n",
        "\n",
        "        self.root = os.path.normpath(root)\n",
        "        self.resize = dimension\n",
        "\n",
        "        mapping_path = os.path.join(os.getcwd(), 'datasets', 'gta5_mapping.json')\n",
        "        self.lb_map = self._load_label_map(mapping_path)\n",
        "\n",
        "        # Define the transform pipeline for images and labels\n",
        "        normalizer = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        self.to_tensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalizer  # Aggiungi la normalizzazione qui\n",
        "        ])\n",
        "        self.to_tensor_label = transforms.PILToTensor()\n",
        "\n",
        "        # List all image and label files\n",
        "        image_files = sorted(os.listdir(os.path.join(self.root, 'images')))\n",
        "        label_files = sorted(os.listdir(os.path.join(self.root, 'labels')))\n",
        "\n",
        "        # Ensure there is a matching number of images and labels\n",
        "        assert len(image_files) == len(label_files), \"Mismatch between number of images and labels.\"\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image_path\": [os.path.join(self.root, 'images', img) for img in image_files],\n",
        "            \"label_path\": [os.path.join(self.root, 'labels', lbl) for lbl in label_files]\n",
        "        })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.data[\"image_path\"].iloc[idx]\n",
        "        label_path = self.data[\"label_path\"].iloc[idx]\n",
        "\n",
        "        # Load and resize image and label\n",
        "        image = Image.open(image_path).resize(self.resize, Image.BILINEAR)\n",
        "        label = Image.open(label_path).resize(self.resize, Image.NEAREST)\n",
        "\n",
        "        # Convert to tensor and normalize the image\n",
        "        image = self.to_tensor(image)\n",
        "        label = self.to_tensor_label(label)\n",
        "        label = self._convert_labels(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def _load_label_map(self, json_path):\n",
        "        with open(json_path, 'r') as fr:\n",
        "            labels_info = json.load(fr)\n",
        "        return {el['id']: el['trainId'] for el in labels_info if 'id' in el}\n",
        "\n",
        "    def _convert_labels(self, label):\n",
        "        # Convert the label tensor to a numpy array, apply mapping, then convert back to tensor\n",
        "        label_np = label.numpy()\n",
        "        label_np = np.vectorize(self.lb_map.get)(label_np)\n",
        "        return torch.tensor(label_np, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_3W77JMuLk9"
      },
      "outputs": [],
      "source": [
        "#@title main cityscape\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "# Dataset: Cityscapes\n",
        "n_classes = 19  # Numero di classi semantiche\n",
        "\n",
        "# Modalità\n",
        "mode = args.mode\n",
        "\n",
        "citySpaces_path =\"C:\\\\Users\\\\miria\\\\Desktop\\\\AML\\\\Cityscapes\\\\Cityspaces\" #os.path.join( \"Desktop\", \"AML\", \"CityScapes\", \"Cityspaces\")\n",
        "pretrainedModel_path = \"C:\\\\Users\\\\miria\\\\Desktop\\\\AML\\\\AML_Semantic_DA-master\\\\pretrained_models\\\\STDCNet813M_73.91\" #os.path.join( \"Desktop\", \"AML\", \"AML_Semantic_DA-master\", \"pretrained_models\", \"STDCNet813M_73.91.tar\")\n",
        "\n",
        "# Dataset di addestramento\n",
        "train_dataset = CityScapesDataset(root_dir=citySpaces_path, mode='train', dimension=(1024, 512))\n",
        "print(\"1111111\")\n",
        "dataloader_train = DataLoader(train_dataset,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=args.num_workers,\n",
        "                              pin_memory=False,\n",
        "                              drop_last=True)\n",
        "print(args.batch_size)\n",
        "print(args.num_workers)\n",
        "\n",
        "# Dataset di validazione\n",
        "val_dataset = CityScapesDataset(root_dir=citySpaces_path, mode='val', dimension=(1024, 512))\n",
        "dataloader_val = DataLoader(val_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            num_workers=args.num_workers,\n",
        "                            drop_last=False)\n",
        "\n",
        "# Modello: STDC pre-addestrato su ImageNet\n",
        "model = BiSeNet(backbone=args.backbone, n_classes=n_classes, pretrain_model=pretrainedModel_path)\n",
        "\n",
        "\n",
        "\n",
        "# Utilizzo della GPU se disponibile\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "## optimizer\n",
        "# build optimizer\n",
        "if args.optimizer == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "elif args.optimizer == 'sgd':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "elif args.optimizer == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "else:  # rmsprop\n",
        "    print('not supported optimizer \\n')\n",
        "\n",
        "\n",
        "## train loop\n",
        "train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "# final test\n",
        "val(args, model, dataloader_val)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use pretrain model c:\\Users\\maxim\\Desktop\\AML_semantic\\pretrained\\STDCNet813M_73.91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_14412\\506475763.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(pretrain_model)[\"state_dict\"]\n",
            "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_14412\\3804231913.py:81: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler()\n",
            "epoch 0, lr 0.010000:   0%|          | 0/16 [00:00<?, ?it/s]C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_14412\\3804231913.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n",
            "epoch 0, lr 0.010000:  50%|█████     | 8/16 [00:06<00:06,  1.28it/s, loss=nan]NaN or Inf found in input tensor.\n",
            "epoch 0, lr 0.010000: 100%|██████████| 16/16 [00:11<00:00,  1.46it/s, loss=nan]NaN or Inf found in input tensor.\n",
            "epoch 0, lr 0.010000: 100%|██████████| 16/16 [00:11<00:00,  1.43it/s, loss=nan]\n",
            "NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss for train : nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1, lr 0.009820:   0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer non supportato.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train loop\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Final test\u001b[39;00m\n\u001b[0;32m     52\u001b[0m val(args, model, dataloader_val)\n",
            "Cell \u001b[1;32mIn[2], line 106\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, optimizer, dataloader_train, dataloader_val)\u001b[0m\n\u001b[0;32m    103\u001b[0m     loss3 \u001b[38;5;241m=\u001b[39m loss_func(out32, label\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2 \u001b[38;5;241m+\u001b[39m loss3\n\u001b[1;32m--> 106\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m    108\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
            "File \u001b[1;32mc:\\Users\\maxim\\Desktop\\AML_semantic\\my_aml\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\maxim\\Desktop\\AML_semantic\\my_aml\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\maxim\\Desktop\\AML_semantic\\my_aml\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main GTA5\n",
        "import os\n",
        "args = parse_args()\n",
        "\n",
        "# Dataset: GTA5\n",
        "n_classes = 19  # Numero di classi semantiche\n",
        "\n",
        "# Modalità\n",
        "mode = args.mode\n",
        "\n",
        "gta5_path = os.path.join(os.getcwd(), 'GTA5_ds')\n",
        "pretrainedModel_path = os.path.join(os.getcwd(), 'pretrained', 'STDCNet813M_73.91')\n",
        "\n",
        "# Dataset di addestramento\n",
        "train_dataset = Gta5Dataset(root=gta5_path, dimension=(1024, 512))\n",
        "dataloader_train = DataLoader(train_dataset,\n",
        "                              batch_size=args.batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=args.num_workers,\n",
        "                              pin_memory=False,\n",
        "                              drop_last=True)\n",
        "\n",
        "# Dataset di validazione (opzionale, usando lo stesso dataset)\n",
        "val_dataset = Gta5Dataset(root=gta5_path, dimension=(1024, 512))\n",
        "dataloader_val = DataLoader(val_dataset,\n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            num_workers=args.num_workers,\n",
        "                            drop_last=False)\n",
        "\n",
        "# Modello: STDC pre-addestrato su ImageNet\n",
        "model = BiSeNet(backbone=args.backbone, n_classes=n_classes, pretrain_model=pretrainedModel_path)\n",
        "\n",
        "# Utilizzo della GPU se disponibile\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Costruzione dell'ottimizzatore\n",
        "if args.optimizer == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "elif args.optimizer == 'sgd':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "elif args.optimizer == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "else:\n",
        "    raise ValueError('Optimizer non supportato.')\n",
        "\n",
        "# Train loop\n",
        "train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "\n",
        "# Final test\n",
        "val(args, model, dataloader_val)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
